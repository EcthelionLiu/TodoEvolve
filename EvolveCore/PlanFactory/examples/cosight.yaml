python_code: |-
  <<<PYTHON>>>
  import textwrap
  import json
  import re
  from typing import Any, Callable, Dict, List, Optional
  from jinja2 import StrictUndefined, Template

  from rich.rule import Rule
  from rich.text import Text

  from .base_planning import BasePlanning
  from ..memory import ActionStep, PlanningStep, SummaryStep
  from ..models import ChatMessage, MessageRole
  from ..monitoring import LogLevel


  def populate_template(template: str, variables: Dict[str, Any]) -> str:
      compiled_template = Template(template, undefined=StrictUndefined)
      try:
          return compiled_template.render(**variables)
      except Exception as e:
          raise Exception(f"Error during jinja template rendering: {type(e).__name__}: {e}")


  class PlannerPlanning(BasePlanning):
      def topology_initialize(self, task: str) -> PlanningStep:
          """
          Let the model decide how many experts (1-4) are needed based on task complexity.
          If this is an internal expert,
          generate a simple research plan via LLM.
          """
          # Expert mode check
          if "cosight_internal" not in self.prompt_templates:
              # Generate a concise plan using the model
              expert_plan_prompt = f"As an expert researcher, provide a very concise, step-by-step research plan (max 3 steps) for this task: {task}\nReturn ONLY the plan text."
              try:
                  resp = self.model([{"role": "user", "content": expert_plan_prompt}])
                  plan_text = getattr(resp, "content", str(resp)).strip()
              except Exception:
                  plan_text = "Analyze task requirements and use available tools to gather and verify evidence."

              self.logger.log(
                  Rule("Expert Plan", style="green"),
                  Text(f"Proposed Research Strategy:\n{plan_text}\n"),
                  level=LogLevel.INFO,
              )

              planning_step = PlanningStep(
                  model_input_messages=[],
                  plan=plan_text,
                  plan_think="",
                  plan_reasoning="Expert-level research strategy generated.",
              )
              self.memory.steps.append(planning_step)
              return planning_step

          # Supervisor mode
          # ... rest of the code ...
          judge_prompt = f"""Analyze the following task and decide how many experts (integer 1-4) are needed to solve it reliably. 
  Consider complexity, potential for conflicting information, and depth of research required.
  Task: {task}
  Return ONLY a JSON object: {{"num_expert": N, "reasoning": "..."}}"""

          try:
              # Use self.model from BasePlanning to judge complexity
              resp = self.model([{"role": "user", "content": judge_prompt}])
              out = getattr(resp, "content", str(resp))
              match = re.search(r"\{.*\}", out, re.DOTALL)
              data = json.loads(match.group(0)) if match else {}
              num_expert = int(data.get("num_expert", 3))
              num_expert = max(1, min(4, num_expert))
              reasoning = data.get("reasoning", "Dynamic expert count based on task complexity.")
          except (AttributeError, ValueError):
              num_expert = 3
              reasoning = "Defaulting to 3 experts due to analysis failure."

          plan_text = json.dumps([{"name": "expert_parallel", "arguments": {"task": task, "num_expert": num_expert}}])
          plan_reasoning = f"Analysis: {reasoning} -> Protocol starts with 'expert_parallel' for {num_expert} experts."

          self.logger.log(
              Rule("Execution", style="orange"),
              Text(f"Complexity analysis complete: Using {num_expert} experts. Initializing expert group...\nReasoning: {reasoning}\nPlan: {plan_text}\n"),
              level=LogLevel.INFO,
          )

          planning_step = PlanningStep(
              model_input_messages=[],
              plan=plan_text,
              plan_think="",
              plan_reasoning=plan_reasoning,
          )
          self.memory.steps.append(planning_step)
          return planning_step

      def adaptation(
          self,
          task: str,
          step: int,
          write_memory_to_messages: Callable[[Optional[List[ActionStep]], Optional[bool]], List[Dict[str, str]]],
      ) -> SummaryStep:

          summary_step = SummaryStep(
              model_input_messages="",
              summary="",
              summary_reasoning="",
          )
          return summary_step
  <<<END_PYTHON>>>
yaml_config: |-
  <<<YAML>>>
  system_prompt: |-
  # Role
  You are a coordination assistant to solve tasks.

  # Protocol Sequence
  1. FIRST: Call `expert_parallel` to gather initial findings.
  2. SECOND: Call `camv` with the results from `expert_parallel`.
  3. FINAL: Call `final_answer` based on `camv` output.

  # Rules
  - Use ONLY the tools provided.
  - Return a JSON list of tool calls. No extra text.
  - MANDATORY ARGUMENTS:
    - `expert_parallel`: must include `task` (string) and `num_expert` (integer).
    - `camv`: must include `task` (string) and `expert_packages` (JSON string).

  planning:
    initial_plan: |-
      Gather diverse perspectives: {"name": "expert_parallel", "arguments": {"task": "{{task}}", "num_expert": 3}}
    static_plan: |-
      [{"name": "expert_parallel", "arguments": {"task": "{{task}}", "num_expert": 3}}]

  step:
    pre_messages: |
      1. expert_parallel(task, num_expert) -> 2. camv(task, expert_packages) -> 3. final_answer

      # Current State
      Your task: {{task}}
      Available tools:
      {{tool_functions_json}}

  final_answer:
    pre_messages: |-
      Provide a short, precise final answer based on verified facts.
    post_messages: |-
      Return JSON: {"answer": "..."}
      Task: {{task}}

  expert_internal:
    system_prompt: |-
      You are an autonomous researcher. Solve tasks using tools.

      # Process
      1. Search/Crawl/Reasoning for evidence.
      2. Resolve conflicts if found.
      3. Call `final_answer` when done.

      # Tools
      {%- for tool in tools.values() %}
      - {{ tool.name }}: {{ tool.description }}
      {%- endfor %}

      # Tool Parameters
      - web_search: {"query": "search query string"}
      - crawl_page: {"url": "URL string", "query": "what to look for on page"}
      - reasoning: {"task": "reasoning task description"}
      - final_answer: {"answer": "final synthesized answer"}

    planning:
      initial_plan: "Decompose task into research steps and verify facts."

    step:
      pre_messages: |-
        Task: {{task}}

        # Rules
        1. Choose tool(s) to advance research. 
        2. You MUST provide the required arguments for each tool (e.g., 'query' for web_search, 'url' for crawl_page, 'task' for reasoning).
        3. Return ONLY the JSON object.

        Tools: {{tool_functions_json}}
        Return JSON: {"think": "...", "tools": [{"name": "...", "arguments": {...}}]}

    final_answer:
      pre_messages: "Provide the final synthesized answer."
      post_messages: |-
        Return JSON: {"think": "...", "answer": "..."}

  cosight_internal:
    expert_planner:
      prompt: |-
        # Role: Expert Researcher {{expert_id}} - Planner
        Task: {{task}}
        Verified Global Facts: {{facts_snapshot}}
        Previous Failures: {{failure_context}}

        # Instructions
        Analyze the task and decompose it into a Directed Acyclic Graph (DAG) of steps.
        Focus on resolving unknowns and verifying assumptions.

        Output the plan as a concise set of objectives.

    expert_react:
      system_prompt: |-
        # Role: Expert {{expert_id}} - TRSF Autonomous Researcher
        You are an autonomous expert agent following the TRSF (Tool -> Notes -> Facts) protocol.

        # Task Context
        Task: "{{task}}"
        Your Plan: {{dag_plan}}

        # TRSF Process-Internal Loop
        1. Action: Call tools to gather evidence.
        2. TRSF Check: Every observation is checked against your current facts module.
        3. Extra Verification: If you detect a conflict or an inconsistency in a tool observation, you MUST spend your next step performing an additional targeted search/crawl to resolve it before proceeding.

        # Instructions
        - Search De-coupling: Split multifaceted tasks into targeted queries.
        - Evidence Mining: Prioritize verified facts and sources from `Global Facts`.
        - When you have enough information or have resolved all local conflicts, call `final_answer`.

        # Context
        - Global Facts (Verified): {{facts_snapshot}}
        - Previous Attempts / EBA Repair: {{failure_context}}

        # Tools Available
        - web_search(query: str): Search the web.
        - crawl_page(url: str, query: str): Extract content from a specific URL.
        - reasoning(task: str): Perform deep reasoning or complex logic.

        # Output Format
        To use a tool, return JSON:
        {"tool": "web_search", "arguments": {"query": "..."}}

        To signal completion, return:
        {"tool": "final_answer", "arguments": {}}

    extract_notes:
      prompt: |-
        # Information Extraction
        Task: {{task}}
        Tool Execution Trajectory:
        {{tool_records}}

        # Instructions
        Extract key evidence snippets and intermediate insights from the trajectory.
        CRITICAL: For every factual claim found, you MUST record the `source_url` and a `source_snippet` of the surrounding text.

        Return JSON: {"notes": ["Note 1 with URL...", "Note 2 with URL...", ...]}

    expert:
      prompt: |-
        # Role: Expert Researcher {{expert_id}} - Fact Extractor
        Task: {{task}}
        Verified Global Facts: {{facts_snapshot}}
        Evidence Gathered in this round: {{notes}}

        # Instructions
        1. Analyze gathered evidence and `Verified Global Facts`.
        2. Formulate a grounded, concise answer.
        3. Extract atomic, verifiable claims from your answer.
        4. For each claim, provide:
           - `key`: standard identifier
           - `value`: the factual value
           - `confidence`: 0.0 to 1.0
           - `source_url`: the EXACT URL where this fact was found
           - `source_snippet`: the EXACT text snippet providing the evidence

        # CRITICAL RULES
        - DO NOT extract negative claims such as "unknown", "not found", "not stated", or "missing".
        - Only extract POSITIVE factual assertions that you have evidence for.
        - If you have no positive facts, return an empty list `[]` for `facts_local`.

        Output STRICT JSON:
        {
          "answer": "...", 
          "facts_local": [
            {
              "key": "entity.property", 
              "value": "fact value", 
              "confidence": 0.9, 
              "source_url": "https://...", 
              "source_snippet": "..."
            },
            ...
          ]
        }

    normalization:
      prompt: |-
        # Fact Normalization
        Task: {{task}}
        Raw Claims:
        {{claims}}

        # Instructions
        1. Map claims to high-level standard keys (`entity.property`).
        2. Preserve qualitative explanations in the "value" field.
        3. Pass through the `confidence`, `source_url`, and `source_snippet` fields.
        4. STRICT FILTERING: Do NOT extract claims that are essentially negative (e.g., "no information", "not found", "unknown"). 
        5. Only discard claims that are completely negative or logically inconsistent. We want to preserve any positive factual assertions with source URLs for verification.

        Return STRICT JSON array: [{"key": "...", "value": "...", "confidence": ..., "source_url": "...", "source_snippet": "..."}]

    verification_query:
      prompt: |-
        # Role: Search Query Optimizer
        Task Context: {{task}}
        Claim to Verify: {{key}} = {{value}}

        # Instructions
        Generate a CONCISE web search query to verify this specific claim. Focus on unique identifiers.
        Return ONLY the query string.

    judge:
      prompt: |-
        # Role: Verification Judge
        Task Context: {{task}}
        Claim to Verify: {{key}} = {{value}}

        # Evidence
        [WEB SEARCH RESULTS]
        {{web_obs}}

        [PAGE CONTENT]
        {{page_obs}}

        # Instructions
        1. Determine if the evidence SUPPORTS, CONTRADICTS, or is NEUTRAL regarding the core claim.
        3. ONLY set `is_contradicted`: true if the evidence explicitly provides a DIFFERENT value for the same key.
        4. If the page is missing or blocked, do NOT contradict; keep it NEUTRAL.
        2. "supported": true if evidence confirms the value.
        3. "is_contradicted": true only if evidence explicitly provides a DIFFERENT value.
        4. We only want positive verified facts.

        Return STRICT JSON:
        {"supported": true/false, "is_contradicted": true/false, "evidence_snippet": "...", "url": "...", "reason": "..."}

    decision:
      prompt: |-
        # Integrative Synthesis (CAMV Stage 4)
        Task: {{task}}
        Verified Fact Anchors:
        {{supported_snapshot}}

        # Instructions
        1. CRITICAL COMPLETENESS CHECK: Does the `supported_snapshot` contain POSITIVE evidence for EVERY single numeric value or factual detail requested in the Task?
        2. If ANY required detail is still "unknown", "missing", or not explicitly present in the verified anchors, you MUST set "ready": false.
        3. DO NOT output a "ready": true decision if the answer would include "not reported", "unknown", or "not found".
        4. If you set "ready": false, use "next_failure_context" to tell experts exactly which specific piece of data (e.g. "the response rate percentage") is missing and ask them to find its source URL.
        5. Act like a puzzle-solver: Reconstruct a coherent reasoning trace leading to the final answer.

        Output JSON: {"ready": true/false, "final_answer": "...", "next_failure_context": "..." }

    fallback:
      prompt: |-
        Task: {{task}}
        Supported facts:
        {{supported_snapshot}}
        Give a best-effort, traceable final answer based ONLY on verified information.

    static_plan: |-
      [{"name": "expert_parallel", "arguments": {"task": "{{task}}", "num_expert": 3}}]

  <<<END_YAML>>>
